{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def FFNN(output_activation,Input_shape,Output_shape,loss_function):\n",
    "    inputs = []\n",
    "    predictions = []\n",
    "    \n",
    "    #Network structure\n",
    "    layer1 = Dense(100, activation='relu')\n",
    "    layer2 = Dense(100, activation='relu')\n",
    "    layer3 = Dense(100, activation='relu')\n",
    "    layer4 = Dense(Output_shape, activation=output_activation)\n",
    "    \n",
    "    #Loops data across trading days through network\n",
    "    for i in range(steps):\n",
    "        sinput = Input(shape=(Input_shape,))\n",
    "        x = layer1(sinput)\n",
    "        x = layer2(x)\n",
    "        x = layer3(x)\n",
    "        sprediction = layer4(x)\n",
    "        inputs.append(sinput)\n",
    "        predictions.append(sprediction)\n",
    "    \n",
    "    #Compiles model\n",
    "    predictions = Concatenate(axis=-1)(predictions)#造still dont know what this does. Understand it.\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer='adam', loss=loss_function, metrics=[])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#this is not used as the results are bad.\n",
    "#造 consider deleting this.\n",
    "def FFNN_sigmoid_linear(output_activation,Input_shape,Output_shape,loss_function):\n",
    "    inputs = []\n",
    "    predictions = []\n",
    "    \n",
    "    #Network structure\n",
    "    layer1 = Dense(100, activation='relu')\n",
    "    layer2 = Dense(100, activation='relu')\n",
    "    layer3 = Dense(100, activation='relu')\n",
    "    layer4 = Dense(Output_shape, activation=output_activation)\n",
    "    \n",
    "    layer4_1 = Dense(1,activation='sigmoid')\n",
    "    layer4_2 = Dense(1,activation='linear')\n",
    "    \n",
    "    #Structure of network\n",
    "    for i in range(steps):\n",
    "        sinput = Input(shape=(Input_shape,))\n",
    "        x = layer1(sinput)\n",
    "        x = layer2(x)\n",
    "        x = layer3(x)\n",
    "        pred_1 = layer4_1(x)\n",
    "        pred_2 = layer4_2(x)\n",
    "        sprediction = tf.concat([pred_1, pred_2],axis=-1)\n",
    "        inputs.append(sinput)#appends input for next timestep. such that the network uses all information up to the given timepoint.\n",
    "        #造is it recurrent now? Or do we need to add predictions to input. something like inputs.append(spredictions)\n",
    "        predictions.append(sprediction)#appends output such that we get a vector for all timepoints in the end.\n",
    "    \n",
    "    #Builds the model\n",
    "    predictions = Concatenate(axis=-1)(predictions)#造still dont know what this does. Understand it.\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer='adam', loss=loss_function, metrics=[])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#final RNN model\n",
    "def RNN_final(output_activation,Input_shape,Output_shape,loss_function):\n",
    "    n_nodes=[100,100,100,Output_shape]\n",
    "    Activation=['relu','relu','relu',output_activation]\n",
    "    n_layers=4 #Hidden layer and output layer.\n",
    "    inputs = Input(shape=(steps,Input_shape))\n",
    "    cells=[tf.keras.layers.SimpleRNNCell(n_nodes[i],activation=Activation[i]) for i in range(n_layers)]\n",
    "    outputs = tf.keras.layers.RNN(tf.keras.layers.StackedRNNCells(cells),\n",
    "                                  stateful=False, \n",
    "                                  return_sequences=True, \n",
    "                                  return_state=False)(inputs)\n",
    "    model=Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam',loss=loss_function)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#this seems to work even better without transaction costs than the RNN function.\n",
    "#and it is assumed to be closer to what the paper does.\n",
    "#with the 'relu' and 'linear' specified then it needs to train for more epochs, but it can get to the right model.\n",
    "#deltas2 are not great though.\n",
    "    #deltas2 are better without 'relu' (Dense has 'linear' by default)\n",
    "    #consider not having 'relu', but its not a simple decision. \n",
    "def RNN_cell(output_activation,Input_shape,Output_shape,loss_function):\n",
    "    n_hidden_units=100\n",
    "    n_hidden_layers=3\n",
    "    inputs = Input(shape=(steps,Input_shape))\n",
    "    cells=[tf.keras.layers.SimpleRNNCell(n_hidden_units,activation='relu') for _ in range(n_hidden_layers)]\n",
    "    outputs = tf.keras.layers.RNN(tf.keras.layers.StackedRNNCells(cells),\n",
    "                              stateful=False, \n",
    "                              return_sequences=True, \n",
    "                              return_state=False)(inputs)\n",
    "    outputs = tf.keras.layers.Dense(Output_shape,activation=output_activation)(outputs)\n",
    "    model=Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam',loss=loss_function)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#This works without transaction costs!\n",
    "#It is however not exactly what the paper does.\n",
    "def RNN(output_activation,Input_shape,Output_shape,loss_function):\n",
    "    model_input=Input(shape=(steps,Input_shape))\n",
    "    x=SimpleRNN(100,return_sequences=True,activation='relu')(model_input)\n",
    "    x=SimpleRNN(100,return_sequences=True,activation='relu')(x)\n",
    "    x=SimpleRNN(100,return_sequences=True,activation='relu')(x)\n",
    "    output=Dense(Output_shape,activation=output_activation)(x)\n",
    "    model=Model(inputs=model_input,outputs=output)\n",
    "    model.compile(optimizer='adam',loss=loss_function)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def RNN_simple(loss_function):\n",
    "    model_input=Input(shape=(steps,3))\n",
    "    model_output=SimpleRNN(2,return_sequences=True)(model_input)\n",
    "    model=Model(inputs=model_input,outputs=model_output)\n",
    "    model.compile(optimizer='adam', loss=loss_function)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}